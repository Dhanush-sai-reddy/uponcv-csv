{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanush-sai-reddy/upondigits/blob/main/Untitled32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq4JqizhqOhb",
        "outputId": "6e677baa-b01a-427e-a7fd-c570016ab730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jcprogjava/handwritten-digits-dataset-not-in-mnist?dataset_version_number=4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 54.6M/54.6M [00:00<00:00, 136MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jcprogjava/handwritten-digits-dataset-not-in-mnist/versions/4\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jcprogjava/handwritten-digits-dataset-not-in-mnist\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owjcxLXbsQnB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpNRdStTsTV7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "GHvHNOz5xR1b",
        "outputId": "5d7ad06d-4376-4bc2-ca7d-86d3fdb89d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digit 0: 10773 images\n",
            "Digit 1: 10773 images\n",
            "Digit 2: 10773 images\n",
            "Digit 3: 10773 images\n",
            "Digit 4: 10773 images\n",
            "Digit 5: 10773 images\n",
            "Digit 6: 10773 images\n",
            "Digit 7: 10773 images\n",
            "Digit 8: 10773 images\n",
            "Digit 9: 10773 images\n",
            "Total: 107730 images\n",
            "Epoch 1: Train Loss=2.3047 Acc=9.86% | Val Loss=2.3028 Acc=9.94%\n",
            "Epoch 2: Train Loss=2.3027 Acc=9.92% | Val Loss=2.3031 Acc=10.20%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2970700415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDigitCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2970700415.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class DigitDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        dataset_path = os.path.join(root_dir, 'dataset') if not root_dir.endswith('dataset') else root_dir\n",
        "\n",
        "        for digit in range(10):\n",
        "            digit_subfolder = os.path.join(dataset_path, str(digit), str(digit))\n",
        "\n",
        "            if os.path.exists(digit_subfolder):\n",
        "                files = [f for f in os.listdir(digit_subfolder) if f.lower().endswith('.png')]\n",
        "                for img_file in files:\n",
        "                    self.images.append(os.path.join(digit_subfolder, img_file))\n",
        "                    self.labels.append(digit)\n",
        "                print(f\"Digit {digit}: {len(files)} images\")\n",
        "\n",
        "        print(f\"Total: {len(self.images)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert('L')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "class DigitCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.drop(x)\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.drop(x)\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.drop(x)\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=20):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (pred == labels).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss={train_loss/len(train_loader):.4f} Acc={100*correct/total:.2f}% | Val Loss={val_loss/len(val_loader):.4f} Acc={100*val_correct/val_total:.2f}%')\n",
        "\n",
        "def segment_digits(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    boxes = sorted([cv2.boundingRect(c) for c in contours], key=lambda x: x[0])\n",
        "\n",
        "    digits = []\n",
        "    for x, y, w, h in boxes:\n",
        "        if w > 5 and h > 5:\n",
        "            pad = 5\n",
        "            digit = gray[max(0,y-pad):min(gray.shape[0],y+h+pad),\n",
        "                        max(0,x-pad):min(gray.shape[1],x+w+pad)]\n",
        "            digits.append(digit)\n",
        "    return digits\n",
        "\n",
        "def predict_number(model, img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    digit_imgs = segment_digits(img)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for d in digit_imgs:\n",
        "        d_pil = Image.fromarray(d)\n",
        "        d_tensor = transform(d_pil).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(d_tensor)\n",
        "            pred = output.argmax(1).item()\n",
        "            result.append(pred)\n",
        "\n",
        "    return int(''.join(map(str, result)))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = DigitDataset(path, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=64)\n",
        "\n",
        "model = DigitCNN().to(device)\n",
        "train(model, train_loader, val_loader, epochs=20)\n",
        "torch.save(model.state_dict(), 'model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "d43qoE6L1Ph4",
        "outputId": "07c33a69-fa16-44d7-bbe0-adc0fa3d2b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digit 0: 10773 images\n",
            "Digit 1: 10773 images\n",
            "Digit 2: 10773 images\n",
            "Digit 3: 10773 images\n",
            "Digit 4: 10773 images\n",
            "Digit 5: 10773 images\n",
            "Digit 6: 10773 images\n",
            "Digit 7: 10773 images\n",
            "Digit 8: 10773 images\n",
            "Digit 9: 10773 images\n",
            "Total: 107730 images\n",
            "\n",
            "Checking sample images...\n",
            "Sample 0: label=0, shape=torch.Size([1, 28, 28]), min=0.000, max=1.000, mean=0.092\n",
            "Sample 1: label=0, shape=torch.Size([1, 28, 28]), min=0.000, max=1.000, mean=0.082\n",
            "Sample 2: label=0, shape=torch.Size([1, 28, 28]), min=0.000, max=1.000, mean=0.075\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAADTCAYAAAAh6HE3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGxdJREFUeJzt3XtwVPX5x/FPEpNNRLIhKAkRwqVSsWBpBYNRUNQMKQUFlFo7teLUUavBERGxdBrxNkakXgaM2qkK2lapOICVVjoOt+oICaDooIJoQVJDotHJJlySQPb7+8OyP+M5gV2y+909m/dr5vtHnpzdfU74JPNw9pyzKcYYIwAAAEtS490AAADoXhg+AACAVQwfAADAKoYPAABgFcMHAACwiuEDAABYxfABAACsYvgAAABWMXwAAACrGD4SwJ49e5SSkqI//OEPUXvO9evXKyUlRevXr4/acwLfRXbhVWQ3vhg+TtCSJUuUkpKiLVu2xLuVmPn888911VVXKScnR9nZ2Zo8ebL+85//xLstdBHZhVeR3eRxUrwbQGLav3+/Lr74YgUCAf3ud79Tenq6HnvsMV100UXatm2bevfuHe8WAVdkF17VnbLL8AFXTz75pHbt2qXq6mqde+65kqQJEyZo+PDheuSRR/Tggw/GuUPAHdmFV3Wn7PK2Swy1tbXp7rvv1siRI+X3+9WjRw+NHTtW69at6/Qxjz32mAYMGKCsrCxddNFF2r59u2ObHTt2aNq0acrNzVVmZqZGjRqlv//978ft5+DBg9qxY4caGhqOu+0rr7yic889N/QLIElDhw7VpZdeqpdffvm4j4e3kV14Fdn1BoaPGGpqatIzzzyjcePGaf78+brnnnv05ZdfqrS0VNu2bXNs/8ILL2jhwoUqKyvT3LlztX37dl1yySWqr68PbfPBBx/ovPPO00cffaTf/va3euSRR9SjRw9NmTJFK1asOGY/1dXVOuuss/TEE08cc7tgMKj3339fo0aNcnyvqKhIn376qZqbm8P7IcCTyC68iux6A2+7xFCvXr20Z88eZWRkhGo33HCDhg4dqkWLFunZZ5/tsP0nn3yiXbt26fTTT5ck/eQnP9Ho0aM1f/58Pfroo5Kk2267TYWFhdq8ebN8Pp8k6ZZbbtGYMWN01113aerUqV3u++uvv1Zra6v69u3r+N7RWm1trc4888wuvxYSE9mFV5Fdb+DIRwylpaWFfgGCwaC+/vprHTlyRKNGjdI777zj2H7KlCmhXwDpm2l39OjR+uc//ynpm3CuXbtWV111lZqbm9XQ0KCGhgZ99dVXKi0t1a5du/T555932s+4ceNkjNE999xzzL4PHTokSaFfsm/LzMzssA2SE9mFV5Fdb2D4iLHnn39eP/zhD5WZmanevXvrtNNO0z/+8Q8FAgHHtkOGDHHUvv/972vPnj2SvpnQjTEqLy/Xaaed1mHNmzdPkvTFF190ueesrCxJUmtrq+N7LS0tHbZB8iK78Cqym/h42yWG/vKXv+i6667TlClTdOedd6pPnz5KS0tTRUWFPv3004ifLxgMSpJmz56t0tJS123OOOOMLvUsSbm5ufL5fNq3b5/je0drBQUFXX4dJC6yC68iu97A8BFDr7zyigYPHqzly5crJSUlVD86LX/Xrl27HLWPP/5YAwcOlCQNHjxYkpSenq6SkpLoN/w/qampOvvss11v5FNVVaXBgwerZ8+eMXt9xB/ZhVeRXW/gbZcYSktLkyQZY0K1qqoqbdy40XX7lStXdnjvsLq6WlVVVZowYYIkqU+fPho3bpz++Mc/uk7HX3755TH7ieSSr2nTpmnz5s0dfhF27typtWvX6mc/+9lxHw9vI7vwKrLrDRz56KLnnntOq1evdtRvu+02TZo0ScuXL9fUqVM1ceJE7d69W08//bR+8IMfaP/+/Y7HnHHGGRozZoxuvvlmtba26vHHH1fv3r01Z86c0DaVlZUaM2aMzj77bN1www0aPHiw6uvrtXHjRv33v//Ve++912mv1dXVuvjiizVv3rzjnvx0yy236E9/+pMmTpyo2bNnKz09XY8++qjy8vJ0xx13hP8DQsIiu/AqspsEDE7I4sWLjaROV01NjQkGg+bBBx80AwYMMD6fz/z4xz82q1atMtOnTzcDBgwIPdfu3buNJLNgwQLzyCOPmP79+xufz2fGjh1r3nvvPcdrf/rpp+baa681+fn5Jj093Zx++ulm0qRJ5pVXXglts27dOiPJrFu3zlGbN29eWPtYU1Njpk2bZrKzs80pp5xiJk2aZHbt2nWiPzIkCLILryK7ySPFmG8dmwIAAIgxzvkAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGBVzO7zUVlZqQULFqiurk4jRozQokWLVFRUdNzHBYNB1dbWqmfPnh3uTgdEwhij5uZmFRQUKDU1shmb7CKeyC68KqLsxuL63aVLl5qMjAzz3HPPmQ8++MDccMMNJicnx9TX1x/3sTU1Nce8jpvFimTV1NSQXZYnF9lleXWFk92YDB9FRUWmrKws9HV7e7spKCgwFRUVx31sY2Nj3H9wrORZjY2NZJflyUV2WV5d4WQ36ud8tLW1aevWrR0+gCc1NVUlJSWu99ZvbW1VU1NTaDU3N0e7JXRjkRxCJrtIJGQXXhVOdqM+fDQ0NKi9vV15eXkd6nl5eaqrq3NsX1FRIb/fH1r9+/ePdktAWMguvIrswmvifrXL3LlzFQgEQqumpibeLQFhIbvwKrKLeIv61S6nnnqq0tLSVF9f36FeX1+v/Px8x/Y+n08+ny/abQARI7vwKrILr4n6kY+MjAyNHDlSa9asCdWCwaDWrFmj4uLiaL8cEDVkF15FduE5EZ1OHaalS5can89nlixZYj788ENz4403mpycHFNXV3fcxwYCgbifqctKnhUIBMguy5OL7LK8usLJbkyGD2OMWbRokSksLDQZGRmmqKjIbNq0KazH8UvAiuaK9A842WUlyiK7LK+ucLKbYowxSiBNTU3y+/3xbgNJIhAIKDs728prkV1EE9mFV4WT3bhf7QIAALoXhg8AAGAVwwcAALCK4QMAAFjF8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArGL4AAAAVjF8AAAAqxg+AACAVQwfAADAKoYPAABgFcMHAACwiuEDAABYxfABAACsOineDSC2MjIyXOvz58931H7xi184aocOHXLUGhoaHLWSkhLX1wkEAsdrEQDQzXDkAwAAWMXwAQAArGL4AAAAVjF8AAAAqzjh1KNSUlIctW3btjlqw4cPD/vxbowxjtrAgQMdNbeTUCVpwIABjlptbW1Yr43ElJrq/n+Wu+++21G74447HLWTTnL+2WlpaXHUnnjiCdfXKS8vP16LQJelpaU5ag888ICjdtNNNzlqJ598sutzvvzyy47atddeewLdeR9HPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWJVi3C5niKOmpib5/f54t5FQ3K5MaWpqctTczrA+cuSI63O6nbXtVnOLh9ut2WfPnu36Ou3t7Y5aVlZWWNtFQyAQUHZ2dkye+7uSMbtz58511O6//37XbcO9gurw4cOOmtvHAASDQdfH9+7d21FLxtv4k117fv7znztqL730kqMWbsYj4fa3z+2KMC8JJ7sc+QAAAFYxfAAAAKsYPgAAgFUMHwAAwCpvn9XSTSxfvtxR69Gjh6PW2NjoqOXm5ka9n7vuustRy8/Pd932V7/6laP25ZdfOmqx6BORcbv187Rp0xw1t5OdJWn8+PGOWnV1dVivPWfOHEftoYcect327bffdtSGDRsW1uuge9u6datr/Zxzzgnr8V999VVYj927d6/r4998801HbcyYMY7avffe66jNmzcvnBY9gyMfAADAKoYPAABgFcMHAACwiuEDAADYZSK0YcMGM2nSJNO3b18jyaxYsaLD94PBoCkvLzf5+fkmMzPTXHrppebjjz8O+/kDgYCRxPrWOnLkiGMFg0HH8vl8juWV3rOzsx0rGq8fCATIrssaNmyYY7n9u+zZs8exbPXY3t7uutwyFe+fZywW2e3aOueccxyrM27Zz83NdaxY9BluP/H+eZ5odjsT8ZGPAwcOaMSIEaqsrHT9/sMPP6yFCxfq6aefVlVVlXr06KHS0lK1tLRE+lJAVJFdeBXZRbKJ+FLbCRMmaMKECa7fM8bo8ccf1+9//3tNnjxZkvTCCy8oLy9PK1eu1NVXX+14TGtrq1pbW0Nfd3YZH9BVZBdeRXaRbKJ6zsfu3btVV1enkpKSUM3v92v06NHauHGj62MqKirk9/tDq3///tFsCQgL2YVXkV14UVSHj7q6OklSXl5eh3peXl7oe981d+5cBQKB0KqpqYlmS0BYyC68iuzCi+J+h1OfzyefzxfvNhKG20c2p6WlOWpuHzf+7cOoiWLVqlWO2uWXX+6oXXnllY7a4sWLY9JTtHg5u5s3b3bUjDGO2sCBAy10466ztwK680e/R4uXsxuuXr16hb3t559/7qh9/fXX0WynU0899ZSjdtNNNzlqbr+Le/bsiUFHdkT1yMfRW2zX19d3qNfX13d6+20gEZBdeBXZhRdFdfgYNGiQ8vPztWbNmlCtqalJVVVVKi4ujuZLAVFFduFVZBdeFPHbLvv379cnn3wS+nr37t3atm2bcnNzVVhYqJkzZ+qBBx7QkCFDNGjQIJWXl6ugoEBTpkyJZt9AxMguvIrsItlEPHxs2bJFF198cejrWbNmSZKmT5+uJUuWaM6cOTpw4IBuvPFGNTY2asyYMVq9erUyMzOj1zVwAsguvIrsItlEPHyMGzfO9cS0o1JSUnTffffpvvvu61JjQLSRXXgV2UWyifvVLujoWH9gvs3tqphE9Prrrztqble7nH322Tbawf9kZWU5at89YTHeTjrJ/c9TuL8j6N769esX9rYrV66MXSPHUVZW5qj98pe/DGu7O++8MyY92cAHywEAAKsYPgAAgFUMHwAAwCqGDwAAYBUnnHpAW1ubo9bZyXiJZsiQIWFtx2dLxMaoUaNc624nbS5dujTW7USks8tEDx48aLkTeNG//vUvR83tYykkae/evbFup1Nuv4tuFxRcccUVjhonnAIAAISJ4QMAAFjF8AEAAKxi+AAAAFZ546zFbq6hocFRO+200xy1tLQ0R629vT0mPYXrmmuuCWu7N998M8addE8jRoxwrbvlory8PNbtdKpv376OWmqq+/+NHn744Vi3gyRQV1fnqO3fv99126Kioli3ExGfz+eo9enTJw6dxA5HPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIoTTj1gx44djprbyUduH0u/bdu2WLQUNrcTY93u6LdlyxYb7XQ7l19+edjbNjc3x7CTY3vvvffC3vb++++PYSdIZm53i5akCy64wHInx+Z2J9YjR47EoZPY4cgHAACwiuEDAABYxfABAACsYvgAAABWccKpB6xcudJRGzt2rKNWXFzsqNk64bSzOwS6fTR0U1NTrNvB/4wePdq13tlHi9swbNgwR+3UU0911F5//XUb7aAb+eCDD1zrP/rRj+w2chwZGRmO2sGDB+PQSexw5AMAAFjF8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFVc7eIBVVVVjlpqqnNuvPrqqx21p556KiY9fdeyZcvC3vavf/1rDDvBt/Xo0cO1Hs9bqb/zzjthbTdx4sQYd4LuZtWqVa71733ve46a25V6bh8N0VWvvfZaWNvNnj076q8dTxz5AAAAVjF8AAAAqxg+AACAVQwfAADAKk449YDq6uqwtjv//PMdNbcTU6Wu3V57/Pjxjlr//v3Dfvytt956wq+NyGRmZrrWDx8+bOX1y8vLHTW3W0cvXrzYRjvo5p555hnX+qxZsxy1yZMnO2puH3URialTpzpqP/3pTx21xsZGRy3Zfkc48gEAAKxi+AAAAFYxfAAAAKsYPgAAgFWccOpRbicf/frXv3bUDh486Pp4txOfdu/e7aitWLHCUTvzzDPDaVGStGDBAketKye7IjpOOeWUqD/nnDlzHLV7773XUWtra3PU3LILRJvbiZyS1KtXL0fN7a7NCxcudNTefvttR23kyJGur3PXXXcdp8NvDBo0KKztvIwjHwAAwCqGDwAAYBXDBwAAsIrhAwAAWJViIviM4IqKCi1fvlw7duxQVlaWzj//fM2fP7/DCYgtLS264447tHTpUrW2tqq0tFRPPvmk8vLywnqNpqYm+f3+yPcErh/NHIuPJXeLzDXXXOO67UsvvRT1149EIBBQdnZ2t83u+vXrXesXXniho3bgwAFHbdOmTY7aRRdd5Pqc6enpjprbnVTd7nAKp+6eXZtycnIctc8++8xRC/dE7c5Oqm9paXHUiouLHbXt27eH9TqJ6mh2jyWiIx8bNmxQWVmZNm3apDfeeEOHDx/W+PHjO/zRuv322/Xaa69p2bJl2rBhg2pra3XFFVec2B4AUUJ24VVkF8kookttV69e3eHrJUuWqE+fPtq6dasuvPBCBQIBPfvss3rxxRd1ySWXSPrmktCzzjpLmzZt0nnnned4ztbWVrW2toa+bmpqOpH9AI6J7MKryC6SUZfO+QgEApKk3NxcSdLWrVt1+PBhlZSUhLYZOnSoCgsLtXHjRtfnqKiokN/vD61IPqAMOFFkF15FdpEMTnj4CAaDmjlzpi644AINHz5cklRXV6eMjAzH+2d5eXmqq6tzfZ65c+cqEAiEVk1NzYm2BISF7MKryC6SxQnf4bSsrEzbt2/XW2+91aUGfD6ffD5fl54D37jssssctYEDB7pu++c//9lRczs58Pnnnw+r5iXdKbvjxo1zrTc3NztqbifTHT2M/22dnaP+3HPPOWrXX3/9cTpEJLpTdm1yu/NpuCfgpqY6/w/vVpOkI0eORNRXMjuhIx8zZszQqlWrtG7dOvXr1y9Uz8/PV1tbm+Mfsr6+Xvn5+V1qFIgGsguvIrtIJhENH8YYzZgxQytWrNDatWsd958fOXKk0tPTtWbNmlBt586d2rt3r+vlRIAtZBdeRXaRjCJ626WsrEwvvviiXn31VfXs2TP0fqLf71dWVpb8fr+uv/56zZo1S7m5ucrOztatt96q4uJi1zOuAVvILryK7CIZRTR8PPXUU5Kc7yMvXrxY1113nSTpscceU2pqqq688soON7sB4onswqvILpJRRMNHODdDzczMVGVlpSorK0+4KSDayC68iuwiGZ3w1S7whj179rjWx44da7cRJJSePXs6agUFBY7a0XtKfJvbbdiB7srtVuqd3V4d/48PlgMAAFYxfAAAAKsYPgAAgFUMHwAAwCpOOAUgSaqtrY13CwC6CY58AAAAqxg+AACAVQwfAADAKoYPAABgFcMHAACwiuEDAABYxfABAACsYvgAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjF8AEAAKxi+AAAAFYxfAAAAKsYPgAAgFUMHwAAwCqGDwAAYBXDBwAAsIrhAwAAWMXwAQAArGL4AAAAViXc8GGMiXcLSCI280R2EU1kF14VTp4Sbvhobm6OdwtIIjbzRHYRTWQXXhVOnlJMgo28wWBQtbW16tmzp5qbm9W/f3/V1NQoOzs73q11WVNTE/tjiTFGzc3NKigoUGqqnRmb7HpHIu8P2Y2uRP63PhGJvD+RZPckSz2FLTU1Vf369ZMkpaSkSJKys7MT7ofcFeyPHX6/3+rrkV3vSdT9IbvRx/7YEW52E+5tFwAAkNwYPgAAgFUJPXz4fD7NmzdPPp8v3q1EBfvTfSTbz4b96T6S7WfD/iSmhDvhFAAAJLeEPvIBAACSD8MHAACwiuEDAABYxfABAACsYvgAAABWJezwUVlZqYEDByozM1OjR49WdXV1vFsK27///W9ddtllKigoUEpKilauXNnh+8YY3X333erbt6+ysrJUUlKiXbt2xafZ46ioqNC5556rnj17qk+fPpoyZYp27tzZYZuWlhaVlZWpd+/eOuWUU3TllVeqvr4+Th0nBq/ml+ySXbKbGJI9vwk5fPztb3/TrFmzNG/ePL3zzjsaMWKESktL9cUXX8S7tbAcOHBAI0aMUGVlpev3H374YS1cuFBPP/20qqqq1KNHD5WWlqqlpcVyp8e3YcMGlZWVadOmTXrjjTd0+PBhjR8/XgcOHAhtc/vtt+u1117TsmXLtGHDBtXW1uqKK66IY9fx5eX8kl2yS3YTQ9Ln1ySgoqIiU1ZWFvq6vb3dFBQUmIqKijh2dWIkmRUrVoS+DgaDJj8/3yxYsCBUa2xsND6fz7z00ktx6DAyX3zxhZFkNmzYYIz5pvf09HSzbNmy0DYfffSRkWQ2btwYrzbjKlnyS3a7H7KbuJItvwl35KOtrU1bt25VSUlJqJaamqqSkhJt3Lgxjp1Fx+7du1VXV9dh//x+v0aPHu2J/QsEApKk3NxcSdLWrVt1+PDhDvszdOhQFRYWemJ/oi2Z80t2kxvZTWzJlt+EGz4aGhrU3t6uvLy8DvW8vDzV1dXFqavoOboPXty/YDComTNn6oILLtDw4cMlfbM/GRkZysnJ6bCtF/YnFpI5v2Q3uZHdxJWM+T0p3g3AO8rKyrR9+3a99dZb8W4FiAjZhZclY34T7sjHqaeeqrS0NMcZu/X19crPz49TV9FzdB+8tn8zZszQqlWrtG7dOvXr1y9Uz8/PV1tbmxobGztsn+j7EyvJnF+ym9zIbmJK1vwm3PCRkZGhkSNHas2aNaFaMBjUmjVrVFxcHMfOomPQoEHKz8/vsH9NTU2qqqpKyP0zxmjGjBlasWKF1q5dq0GDBnX4/siRI5Went5hf3bu3Km9e/cm5P7EWjLnl+wmN7KbWJI+v3E+4dXV0qVLjc/nM0uWLDEffvihufHGG01OTo6pq6uLd2thaW5uNu+++6559913jSTz6KOPmnfffdd89tlnxhhjHnroIZOTk2NeffVV8/7775vJkyebQYMGmUOHDsW5c6ebb77Z+P1+s379erNv377QOnjwYGib3/zmN6awsNCsXbvWbNmyxRQXF5vi4uI4dh1fXs4v2SW7ZDcxJHt+E3L4MMaYRYsWmcLCQpORkWGKiorMpk2b4t1S2NatW2ckOdb06dONMd9c9lVeXm7y8vKMz+czl156qdm5c2d8m+6E235IMosXLw5tc+jQIXPLLbeYXr16mZNPPtlMnTrV7Nu3L35NJwCv5pfskl2ymxiSPb8pxhgT22MrAAAA/y/hzvkAAADJjeEDAABYxfABAACsYvgAAABWMXwAAACrGD4AAIBVDB8AAMAqhg8AAGAVwwcAALCK4QMAAFjF8AEAAKz6P+VPGj+gf78sAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training: 1347 batches per epoch\n",
            "Epoch 1: Train Loss=0.0813 Acc=97.40% | Val Loss=0.0001 Acc=100.00%\n",
            "Epoch 2: Train Loss=0.0041 Acc=99.85% | Val Loss=0.0003 Acc=99.99%\n",
            "Epoch 3: Train Loss=0.0056 Acc=99.83% | Val Loss=0.0011 Acc=99.96%\n",
            "Epoch 4: Train Loss=0.0050 Acc=99.85% | Val Loss=0.0002 Acc=100.00%\n",
            "Epoch 5: Train Loss=0.0026 Acc=99.92% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 6: Train Loss=0.0032 Acc=99.92% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 7: Train Loss=0.0023 Acc=99.94% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 8: Train Loss=0.0026 Acc=99.93% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 9: Train Loss=0.0028 Acc=99.93% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 10: Train Loss=0.0010 Acc=99.96% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 11: Train Loss=0.0016 Acc=99.95% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 12: Train Loss=0.0035 Acc=99.92% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 13: Train Loss=0.0005 Acc=99.98% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 14: Train Loss=0.0014 Acc=99.96% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 15: Train Loss=0.0020 Acc=99.95% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 16: Train Loss=0.0015 Acc=99.97% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 17: Train Loss=0.0008 Acc=99.99% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 18: Train Loss=0.0003 Acc=99.99% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 19: Train Loss=0.0014 Acc=99.97% | Val Loss=0.0000 Acc=100.00%\n",
            "Epoch 20: Train Loss=0.0019 Acc=99.96% | Val Loss=0.0000 Acc=100.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class DigitDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        dataset_path = os.path.join(root_dir, 'dataset') if not root_dir.endswith('dataset') else root_dir\n",
        "\n",
        "        for digit in range(10):\n",
        "            digit_subfolder = os.path.join(dataset_path, str(digit), str(digit))\n",
        "\n",
        "            if os.path.exists(digit_subfolder):\n",
        "                files = [f for f in os.listdir(digit_subfolder) if f.lower().endswith('.png')]\n",
        "                for img_file in files:\n",
        "                    self.images.append(os.path.join(digit_subfolder, img_file))\n",
        "                    self.labels.append(digit)\n",
        "                print(f\"Digit {digit}: {len(files)} images\")\n",
        "\n",
        "        print(f\"Total: {len(self.images)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx])\n",
        "\n",
        "        if img.mode == 'RGBA':\n",
        "            bg = Image.new('RGB', img.size, (255, 255, 255))\n",
        "            bg.paste(img, mask=img.split()[3])\n",
        "            img = bg\n",
        "\n",
        "        img = img.convert('L')\n",
        "        img = Image.fromarray(255 - np.array(img))\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "class DigitCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.drop(x)\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.drop(x)\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.drop(x)\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(f\"Starting training: {len(train_loader)} batches per epoch\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (pred == labels).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss={train_loss/len(train_loader):.4f} Acc={100*correct/total:.2f}% | Val Loss={val_loss/len(val_loader):.4f} Acc={100*val_correct/val_total:.2f}%')\n",
        "\n",
        "def segment_digits(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    boxes = sorted([cv2.boundingRect(c) for c in contours], key=lambda x: x[0])\n",
        "\n",
        "    digits = []\n",
        "    for x, y, w, h in boxes:\n",
        "        if w > 5 and h > 5:\n",
        "            pad = 5\n",
        "            digit = gray[max(0,y-pad):min(gray.shape[0],y+h+pad),\n",
        "                        max(0,x-pad):min(gray.shape[1],x+w+pad)]\n",
        "            digits.append(digit)\n",
        "    return digits\n",
        "\n",
        "def predict_number(model, img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    digit_imgs = segment_digits(img)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for d in digit_imgs:\n",
        "        d_pil = Image.fromarray(d)\n",
        "        d_tensor = transform(d_pil).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(d_tensor)\n",
        "            pred = output.argmax(1).item()\n",
        "            result.append(pred)\n",
        "\n",
        "    return int(''.join(map(str, result)))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = DigitDataset(path, transform=transform)\n",
        "\n",
        "for i in range(3):\n",
        "    img, label = dataset[i]\n",
        "    print(f\"Sample {i}: label={label}, shape={img.shape}, min={img.min():.3f}, max={img.max():.3f}, mean={img.mean():.3f}\")\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.imshow(img.squeeze(), cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "plt.show()\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=64)\n",
        "\n",
        "model = DigitCNN().to(device)\n",
        "train(model, train_loader, val_loader, epochs=20)\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "# model.load_state_dict(torch.load('model.pth'))\n",
        "# number = predict_number(model, 'test_67.png')\n",
        "# print(number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPoRL7sguySZD1R0zW6Efjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
